{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891bbb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 [0/52500] Loss: 2.3067\n",
      "Train Epoch 0 [6400/52500] Loss: 2.3046\n",
      "Train Epoch 0 [12800/52500] Loss: 2.3047\n",
      "Train Epoch 0 [19200/52500] Loss: 2.3027\n",
      "Train Epoch 0 [25600/52500] Loss: 2.2995\n",
      "Train Epoch 0 [32000/52500] Loss: 2.3026\n",
      "Train Epoch 0 [38400/52500] Loss: 2.3026\n",
      "Train Epoch 0 [44800/52500] Loss: 2.3030\n",
      "Train Epoch 0 [51200/52500] Loss: 2.3042\n",
      "\n",
      "Test set: Avg loss: 2.3020\n",
      " Head 0: Accuracy: 11.50%\n",
      " Head 1: Accuracy: 11.50%\n",
      " Head 2: Accuracy: 10.07%\n",
      " Head 3: Accuracy: 9.50%\n",
      " Head 4: Accuracy: 9.53%\n",
      " Head 5: Accuracy: 9.71%\n",
      " Head 6: Accuracy: 9.89%\n",
      "\n",
      "Train Epoch 1 [0/52500] Loss: 2.3031\n",
      "Train Epoch 1 [6400/52500] Loss: 2.2985\n",
      "Train Epoch 1 [12800/52500] Loss: 2.3041\n",
      "Train Epoch 1 [19200/52500] Loss: 2.3023\n",
      "Train Epoch 1 [25600/52500] Loss: 2.2974\n",
      "Train Epoch 1 [32000/52500] Loss: 2.2978\n",
      "Train Epoch 1 [38400/52500] Loss: 2.3017\n",
      "Train Epoch 1 [44800/52500] Loss: 2.2985\n",
      "Train Epoch 1 [51200/52500] Loss: 2.2970\n",
      "\n",
      "Test set: Avg loss: 2.3010\n",
      " Head 0: Accuracy: 11.50%\n",
      " Head 1: Accuracy: 13.30%\n",
      " Head 2: Accuracy: 12.04%\n",
      " Head 3: Accuracy: 16.24%\n",
      " Head 4: Accuracy: 16.99%\n",
      " Head 5: Accuracy: 15.29%\n",
      " Head 6: Accuracy: 11.19%\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/yam/Multi_Dim_CP/outputs/test_outputs_dataset_name.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 266\u001b[0m\n\u001b[1;32m    264\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/storage/yam/Multi_Dim_CP/outputs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    265\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClutteredMNIST\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/test_outputs_dataset_name.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cal_outputs_dataset_name.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, softmax(cal_output,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    268\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cal_target_dataset_name.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, cal_targets)\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:580\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    579\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 580\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    583\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage/yam/Multi_Dim_CP/outputs/test_outputs_dataset_name.npy'"
     ]
    }
   ],
   "source": [
    "# Train a new model\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from scipy.special import softmax\n",
    "\n",
    "# === CHAT'S CODE ====\n",
    "# Basic configurable multi-head CNN\n",
    "# Parameters:\n",
    "# - input_channels: Number of channels in input images\n",
    "# - num_classes: Number of output classes per head\n",
    "# - heads_num: Number of classification heads\n",
    "# - conv_layers_config: List of integers, number of filters per conv layer\n",
    "# - avg_pool_spatial_size: Integer, number of spatial dimensions after average pooling\n",
    "# - fc_layers_config: List of integers, number of neurons per fully connected layer IN EACH HEAD, NOT including linear classifier layer (last one)\n",
    "# - dropout_prob: Dropout probability in FC layers\n",
    "class MultiHeadCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=10, heads_num=3, conv_layers_config=[16, 32], avg_pool_spatial_size=1, fc_layers_config=[], dropout_prob=0.2):\n",
    "        super(MultiHeadCNN, self).__init__()\n",
    "        \n",
    "        # Build convolutional backbone\n",
    "        layers = []\n",
    "        in_ch = input_channels\n",
    "        for out_ch in conv_layers_config:\n",
    "            layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "            in_ch = out_ch\n",
    "        layers.append(nn.AdaptiveAvgPool2d((avg_pool_spatial_size,avg_pool_spatial_size)))\n",
    "        layers.append(nn.Flatten())\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        # Flatten size after conv layers (assuming square input)\n",
    "        self.flatten_size = in_ch * avg_pool_spatial_size * avg_pool_spatial_size  # placeholder, will compute dynamically in forward if needed\n",
    "        \n",
    "        # Build multi-heads\n",
    "        self.classification_heads = nn.ModuleList()\n",
    "        for _ in range(heads_num):\n",
    "            head_layers = []\n",
    "            prev_size = self.flatten_size\n",
    "            for fc_size in fc_layers_config:\n",
    "                head_layers.append(nn.Linear(prev_size, fc_size))\n",
    "                head_layers.append(nn.ReLU())\n",
    "                head_layers.append(nn.Dropout(dropout_prob))\n",
    "                prev_size = fc_size\n",
    "            head_layers.append(nn.Linear(prev_size, num_classes))\n",
    "            self.classification_heads.append(nn.Sequential(*head_layers))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional backbone\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Pass through each head\n",
    "        outputs = []\n",
    "        for head in self.classification_heads:\n",
    "            outputs.append(head(x))\n",
    "        return outputs\n",
    "\n",
    "# A harder variation of MNIST with squiggles around the digit\n",
    "class ClutteredMNIST(Dataset):\n",
    "    def __init__(self, root=\"./data\", train=True, download=True,\n",
    "                 canvas_size=48, clutter_elements=8, clutter_size=8, transform=None):\n",
    "        \"\"\"\n",
    "        canvas_size: output image size (square)\n",
    "        clutter_elements: number of distractor patches per image\n",
    "        clutter_size: size of the clutter patches (square)\n",
    "        \"\"\"\n",
    "        self.mnist = datasets.MNIST(root, train=train, download=download)\n",
    "        self.canvas_size = canvas_size\n",
    "        self.clutter_elements = clutter_elements\n",
    "        self.clutter_size = clutter_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.mnist[idx]\n",
    "        img = np.array(img)\n",
    "\n",
    "        # Create blank canvas\n",
    "        canvas = np.zeros((self.canvas_size, self.canvas_size), dtype=np.uint8)\n",
    "\n",
    "        # Place digit roughly in center\n",
    "        x_offset = (self.canvas_size - 28) // 2\n",
    "        y_offset = (self.canvas_size - 28) // 2\n",
    "        canvas[y_offset:y_offset+28, x_offset:x_offset+28] = img\n",
    "\n",
    "        # Add clutter patches\n",
    "        center_taken = False\n",
    "        for _ in range(self.clutter_elements):\n",
    "            # pick random digit image\n",
    "            rand_img, _ = self.mnist[random.randint(0, len(self.mnist)-1)]\n",
    "            rand_img = np.array(rand_img)\n",
    "\n",
    "            # crop a random patch\n",
    "            x0 = random.randint(0, 28 - self.clutter_size)\n",
    "            y0 = random.randint(0, 28 - self.clutter_size)\n",
    "            patch = rand_img[y0:y0+self.clutter_size, x0:x0+self.clutter_size]\n",
    "\n",
    "            # paste it at a random location on canvas\n",
    "            if center_taken:\n",
    "                cx = random.choice([random.randint(0, x_offset-self.clutter_size),\n",
    "                                    random.randint(x_offset+28, self.canvas_size-self.clutter_size)])\n",
    "                cy = random.choice([random.randint(0, x_offset-self.clutter_size),\n",
    "                                    random.randint(x_offset+28, self.canvas_size-self.clutter_size)])\n",
    "            else:\n",
    "                cx = random.randint(0, self.canvas_size - self.clutter_size)\n",
    "                cy = random.randint(0, self.canvas_size - self.clutter_size)\n",
    "                if x_offset < cx < x_offset+28-self.clutter_size and y_offset < cy < y_offset+28-self.clutter_size:\n",
    "                    center_taken = True\n",
    "            canvas[cy:cy+self.clutter_size, cx:cx+self.clutter_size] = patch\n",
    "\n",
    "        # Convert to tensor\n",
    "        if self.transform:\n",
    "            canvas = self.transform(canvas)\n",
    "\n",
    "        return canvas, target\n",
    "\n",
    "# Load dataset and split into train, val, test, and calibration sets.\n",
    "def load_mnist(batch_size=64,\n",
    "               train_size_p=0.65,\n",
    "               val_size_p=0.1,\n",
    "               test_size_p=0.1,\n",
    "               cal_size_p=0.15,\n",
    "               data_folder='./data',\n",
    "               cluttered=True):\n",
    "\n",
    "    torch.manual_seed(5)  # reproducibility\n",
    "\n",
    "    # Transform for MNIST\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Download MNIST\n",
    "    if not cluttered:\n",
    "        train_set = datasets.MNIST(root=data_folder, train=True, download=True, transform=transform)\n",
    "        test_set = datasets.MNIST(root=data_folder, train=False, download=True, transform=transform)\n",
    "    else:\n",
    "        train_set = ClutteredMNIST(train=True, transform=transform)\n",
    "        test_set = ClutteredMNIST(train=False, transform=transform)\n",
    "\n",
    "    # Combine train and test into one dataset for custom splitting\n",
    "    dataset = ConcatDataset([train_set, test_set])\n",
    "\n",
    "    # Compute split sizes\n",
    "    num_samples = len(dataset)\n",
    "    train_size = int(train_size_p * num_samples)\n",
    "    val_size = int(val_size_p * num_samples)\n",
    "    test_size = int(test_size_p * num_samples)\n",
    "    cal_size = int(cal_size_p * num_samples)\n",
    "    remaining_size = num_samples - (train_size + val_size + test_size + cal_size)\n",
    "\n",
    "    # Split dataset\n",
    "    train_set, val_set, test_set, cal_set, remaining_set = random_split(\n",
    "        dataset, [train_size, val_size, test_size, cal_size, remaining_size]\n",
    "    )\n",
    "\n",
    "    # Merge leftover into train\n",
    "    train_set = ConcatDataset([train_set, remaining_set])\n",
    "\n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    cal_loader = DataLoader(cal_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, cal_loader\n",
    "\n",
    "def evaluate_individual_heads(model, data_loader, criterion, device, heads_num=7):\n",
    "    model.eval()\n",
    "    running_losses = np.zeros(heads_num)\n",
    "    corrects = [0] * heads_num\n",
    "    results=[]\n",
    "    count=-1\n",
    "    targets=[]\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            all_outputs = model(data)\n",
    "            for i, outputs in enumerate(all_outputs):\n",
    "                loss = criterion(outputs, target.long())\n",
    "                running_losses[i] += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                corrects[i] += predicted.eq(target).sum().item()\n",
    "                if count == -1:\n",
    "                    results.append(outputs.cpu().numpy())\n",
    "                else:\n",
    "                    results[i] = np.concatenate([results[i], outputs.cpu().numpy()], axis=0)\n",
    "            count = 0\n",
    "            targets = np.concatenate([targets, target.cpu().numpy()], axis=0)\n",
    "\n",
    "    losses =  np.array(running_losses) / len(data_loader)\n",
    "    accuracies = (np.array(corrects)/results[0].shape[0]).tolist()\n",
    "    return losses, accuracies,np.array(results),targets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 64\n",
    "    heads_num = 7 # Number of heads to train\n",
    "\n",
    "    train_loader, val_loader, test_loader, cal_loader = load_mnist(batch_size=batch_size, val_size_p=0.0, cluttered=True)\n",
    "    model = MultiHeadCNN(input_channels=1,\n",
    "                         num_classes=10,\n",
    "                         heads_num=heads_num,\n",
    "                         conv_layers_config=[64, 128, 256],\n",
    "                         avg_pool_spatial_size=1,\n",
    "                         fc_layers_config=[256],\n",
    "                         dropout_prob=0.2).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(2):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)  # list of [batch, num_classes]\n",
    "            \n",
    "            # Average loss over all heads\n",
    "            loss = sum(criterion(out, target) for out in outputs) / len(outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Train Epoch {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] \"\n",
    "                    f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = [0]*len(model.classification_heads)\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                \n",
    "                test_loss += data.shape[0] * sum(criterion(out, target) for out in outputs) / len(outputs)\n",
    "                for i, out in enumerate(outputs):\n",
    "                    #test_loss += criterion(out, target).item()\n",
    "                    pred = out.argmax(dim=1)\n",
    "                    correct[i] += pred.eq(target).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accs = [100. * c / len(test_loader.dataset) for c in correct]\n",
    "        print(f\"\\nTest set: Avg loss: {test_loss:.4f}\")\n",
    "        for i, acc in enumerate(accs):\n",
    "            print(f\" Head {i}: Accuracy: {acc:.2f}%\")\n",
    "        print()\n",
    "\n",
    "# Save outputs\n",
    "test_loss, test_accuracy,test_output,test_targets = evaluate_individual_heads(model, test_loader, criterion, device)\n",
    "cal_loss, cal_accuracy,cal_output,cal_targets = evaluate_individual_heads(model, cal_loader, criterion, device)\n",
    "save_dir = 'outputs'\n",
    "dataset_name = 'ClutteredMNIST'\n",
    "np.save(f\"{save_dir}/test_outputs_{dataset_name}.npy\", softmax(test_output,axis=2))\n",
    "np.save(f\"{save_dir}/cal_outputs_{dataset_name}.npy\", softmax(cal_output,axis=2))\n",
    "np.save(f\"{save_dir}/cal_target_{dataset_name}.npy\", cal_targets)\n",
    "np.save(f\"{save_dir}/test_target_{dataset_name}.npy\", test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc26bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, compute_scores, generate_Dcal_Dcells_sets, create_random_split\n",
    "from multi_dim_cp import main_algo\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from matplotlib.patches import Patch\n",
    "from utils import create_true_rest_sets\n",
    "from tabulate import tabulate\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "config={\n",
    "    'DATASET_NAME': 'PathMNIST_demo', # CIFAR100_demo, PathMNIST_demo\n",
    "    \n",
    "    'ALPHA': 0.2,\n",
    "    \n",
    "    'b': 1, # Always 1\n",
    "    \n",
    "    'N_HEADS': [1,2,4,7],\n",
    "    \n",
    "    'SCORING_METHOD': 'RAPS' # RAPS, SAPS , NAIVE , APS\n",
    "    \n",
    "}\n",
    "\n",
    "cal_output,cal_target,test_output,test_target=load_data(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
